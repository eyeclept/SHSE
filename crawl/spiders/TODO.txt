next step: basic scraper

https://www.youtube.com/watch?v=0LTXCcVRQi0
gen url frontier
	got genurls as a base, need to integrate with crawler
	add dns lookup to ensure collected urls are in private ip ranges, then have a list of urls not in private ranges set to ignore
	robots.txt cache
	needs priority system for crawling continuously
	needs option to crawl entire range, and option to just keep things up to date
	needs politeness to ensure only 1 crawler crawls each site 
		multiple queues with priorities and % chance to pick url from que
		queues based on hosts
set up scraper
	set max number of spiders (8 or 16 maybe)
figure out database
	need metadata database
		holds urls, hashes, and text index
	need site content database
other things
	close duplicates? (shingles)
